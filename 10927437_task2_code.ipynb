{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This section prints out the runtime configuration.\n",
        "# Please don't edit this code block and keep this in your code.\n",
        "# When submitting your notebook, please ensure this block is executed as well\n",
        "# Please ignore all error messages in this block.\n",
        "# For course work 1, please only use CPU for calculation. You can change it to CPU only by selecting:\n",
        "# Runtime -> Change runtime type -> Hardware accelerator: None (or select GPU/TPU as required).\n",
        "!cat /proc/cpuinfo | grep \"model name\" | uniq\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-IIudcNDY6h",
        "outputId": "4d38f3eb-7b36-49ea-cf97-40a578de34b5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV8wcjeXEJrc",
        "outputId": "6cea5825-e645-4393-9719-4f5010d46bc5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "rOigN65O4hYo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6UXopqG48bZ",
        "outputId": "af59f4ac-0cd2-4d50-ea73-1eef2dda2d8b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./data/WikiText-103.txt', 'r') as file:\n",
        "  corpus = file.read()\n",
        "\n",
        "# Regex pattern to split the corpus\n",
        "pattern = r'(?<!\\=)\\= [^\\=]+ \\= (?!\\=)'\n",
        "articles = re.split(pattern, corpus)\n",
        "len(articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK9OmHFj41iP",
        "outputId": "7e787c3f-2efd-4c5b-bd4e-dfd590d4bf96"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10728"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a fucntion to clean the text\n",
        "def clean_text(text):\n",
        "  # Convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove punctuation\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  # Remove numbers\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "  # Remove newline characters\n",
        "  text = re.sub('\\n', '', text)\n",
        "\n",
        "  # Remove square brackets\n",
        "  text = re.sub('\\[.*?\\]', ' ', text)\n",
        "\n",
        "  # Remove HTML tags\n",
        "  text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "  # Remove urls\n",
        "  text = re.sub('<.*?>+', ' ', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "articles = [clean_text(article) for article in articles]\n",
        "len(articles)"
      ],
      "metadata": {
        "id": "o3YIQu5h5Pp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7e4ea8-b3c9-4085-a7a4-ba4bac237008"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10728"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initalize WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define the stop words set\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Funtion to tokenize, lemmatize and remove stop words\n",
        "def process_text(list_of_strings):\n",
        "\n",
        "  processed_strings = []\n",
        "\n",
        "  for text in list_of_strings:\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lemmatize each token and remove stop words\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Append the processed sentence to the result list\n",
        "    processed_strings.append(lemmatized_tokens)\n",
        "\n",
        "  return processed_strings\n",
        "\n",
        "# Process the articles\n",
        "tokenized_articles = process_text(articles)"
      ],
      "metadata": {
        "id": "YNsUiYSTbay0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Word2Vec model using Skip-gram\n",
        "word2vec_model = Word2Vec(sentences=tokenized_articles, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "word2vec_model.save(\"word2vec_model.model\")\n",
        "\n",
        "# Size of vocabulary\n",
        "print(\"Size of vocabulary:\", len(word2vec_model.wv))"
      ],
      "metadata": {
        "id": "CqK9EB9d_OsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b8a114-fd39-4d24-9454-e7f2e2add22a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary: 154891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve the vector representation of a token\n",
        "def get_vector(word):\n",
        "  # If the word is in the vocabulary, return its vector\n",
        "  if word in word2vec_model.wv.key_to_index:\n",
        "    word_array = word.split()\n",
        "    vectors = [word2vec_model.wv[w] for w in word_array]\n",
        "\n",
        "    # Handling Out-of-Vocabulary (OOV) words by returning None\n",
        "    if len(vectors) == 0:\n",
        "      return None\n",
        "\n",
        "    return np.mean(vectors, axis=0).reshape(1, -1)\n",
        "\n",
        "  # Handling Out-of-Vocabulary (OOV) words by returning None\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "L9fp_1n0ATrR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the cosine similarity matrix between two lists of tokens\n",
        "def calculate_similarity_matrix(tokens1, tokens2):\n",
        "  similarities = []\n",
        "  for token1 in tokens1:\n",
        "    for token2 in tokens2:\n",
        "      # Retrieve vectors for each pair of tokens\n",
        "      vector1 = get_vector(token1)\n",
        "      vector2 = get_vector(token2)\n",
        "\n",
        "      # Handling OOV words by assigning a similarity of 0.5\n",
        "      if vector1 is None or vector2 is None:\n",
        "        similarities.append(0.5)\n",
        "      else:\n",
        "        # Reshape the vectors for cosine similarity calculation\n",
        "        vector1 = vector1.reshape(1, -1)\n",
        "        vector2 = vector2.reshape(1, -1)\n",
        "\n",
        "        # Calculate cosine similarity and extract the scalar value from the matrix\n",
        "        cosine_similarity_matrix = cosine_similarity(vector1, vector2)[0, 0]\n",
        "\n",
        "        similarities.append(cosine_similarity_matrix)\n",
        "\n",
        "  return similarities"
      ],
      "metadata": {
        "id": "f4aPj-A0AXX3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the consine similarity matrix between two terms\n",
        "def get_cosine_similarity(term1, term2):\n",
        "  # Check if either term contains spaces, indicating multi-word phrases\n",
        "  if (' ' in term1 or ' ' in term2):\n",
        "    # Calculate the similarity for every pair of words in term1 and term2\n",
        "    term1_tokens = term1.split()\n",
        "    term2_tokens = term2.split()\n",
        "\n",
        "    similarities = calculate_similarity_matrix(term1_tokens, term2_tokens)\n",
        "\n",
        "    # All tokens are OOV, handling by returning a default similarity of 0.5\n",
        "    if not similarities:\n",
        "        return 0.5\n",
        "\n",
        "    # Return the average similarity for multi-word phrases\n",
        "    return np.mean(similarities)\n",
        "\n",
        "  else:\n",
        "    # Calculate similarity between single words\n",
        "    vector1 = get_vector(term1)\n",
        "    vector2 = get_vector(term2)\n",
        "\n",
        "    if vector1 is None or vector2 is None:\n",
        "        # Handling OOV words by returning a default similarity of 0.5\n",
        "        return 0.5\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_similarity_matrix = cosine_similarity(vector1, vector2)\n",
        "\n",
        "    # Extract the scalar value from the matrix\n",
        "    cosine_similarity_value = cosine_similarity_matrix[0, 0]\n",
        "\n",
        "    # Return the similarity between single words\n",
        "    return cosine_similarity_value"
      ],
      "metadata": {
        "id": "XXv-7SfaJ0z1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a DataFrame with specified column names\n",
        "validation_df = pd.read_csv(\"./data/CW-1-testdata.csv\", header=None)\n",
        "\n",
        "validation_df.columns = [\"id\",\"term1\", \"term2\"]\n",
        "\n",
        "# Clean the input values\n",
        "validation_df[[\"term1\", \"term2\"]] = validation_df[[\"term1\", \"term2\"]].map(clean_text)\n",
        "\n",
        "# Produce the output\n",
        "validation_df[\"cosine_similarity\"] = validation_df.apply(lambda x: get_cosine_similarity(x[\"term1\"], x[\"term2\"]), axis=1)\n",
        "\n",
        "# Create a .csv file\n",
        "validation_df.to_csv(\"./data/10927437_task2_results.csv\", header=False, index=False)"
      ],
      "metadata": {
        "id": "u_nxYJUbEXPv"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}