{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNNmDpeuYyEK",
        "outputId": "76fb8426-8ebb-4961-df74-c3246758983f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "# This section prints out the runtime configuration.\n",
        "# Please don't edit this code block and keep this in your code.\n",
        "# When submitting your notebook, please ensure this block is executed as well\n",
        "# Please ignore all error messages in this block.\n",
        "# For course work 1, please only use CPU for calculation. You can change it to CPU only by selecting:\n",
        "# Runtime -> Change runtime type -> Hardware accelerator: None (or select GPU/TPU as required).\n",
        "!cat /proc/cpuinfo | grep \"model name\" | uniq\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XZee5NkWbVZR"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnIBBs6jCpj0",
        "outputId": "5a0757a1-2294-4c48-e7c7-1e2f47d66715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mIVSCxicOMjh"
      },
      "outputs": [],
      "source": [
        "with open('./data/WikiText-103.txt', 'r') as file:\n",
        "  corpus = file.read()\n",
        "\n",
        "# Regex pattern to split the corpus\n",
        "pattern = r'(?<!\\=)\\= [^\\=]+ \\= (?!\\=)'\n",
        "articles = re.split(pattern, corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_MnpPUrn06U2"
      },
      "outputs": [],
      "source": [
        "# Defining a fucntion to clean the text\n",
        "def clean_text(text):\n",
        "  # Convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove punctuation\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  # Remove numbers\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "  # Remove newline characters\n",
        "  text = re.sub('\\n', '', text)\n",
        "\n",
        "  # Remove square brackets\n",
        "  text = re.sub('\\[.*?\\]', '', text)\n",
        "\n",
        "  # Remove HTML tags\n",
        "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "  # Remove urls\n",
        "  text = re.sub('<.*?>+', '', text)\n",
        "\n",
        "  # Tokenization\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  text = ' '.join(tokens)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8f4suIBTJgs",
        "outputId": "64cef162-7d57-4962-9fe0-da6a4c064a27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10728"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Clean the article\n",
        "articles = [clean_text(article) for article in articles]\n",
        "len(articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm0RnCj35QOy",
        "outputId": "a5a6075d-f74a-4f35-83b4-c40cc262e7e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary: 428434\n",
            "Dimensions of feature matrix: (10728, 428434)\n"
          ]
        }
      ],
      "source": [
        "# Define the stop words list\n",
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "# Using TFIDF to get the vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words=stop_words, sublinear_tf=True, ngram_range=(1, 3), min_df=5, max_df=0.3)\n",
        "features = tfidf_vectorizer.fit_transform(articles)\n",
        "\n",
        "# Size of vocabulary\n",
        "print(\"Size of vocabulary:\" , len(tfidf_vectorizer.get_feature_names_out()))\n",
        "\n",
        "# Dimensions of feature matrix\n",
        "print(\"Dimensions of feature matrix:\" , features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve the vector representation of a token\n",
        "def get_vector(token):\n",
        "  # If the token is in the vocabulary, return its vector\n",
        "  if token in tfidf_vectorizer.vocabulary_:\n",
        "    index = tfidf_vectorizer.vocabulary_[token]\n",
        "    return features[:, index].toarray()\n",
        "  # Handling Out-of-Vocabulary (OOV) words by returning None\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "fMsvHuo5QzOK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the cosine similarity matrix between two lists of tokens\n",
        "def calculate_similarity_matrix(tokens_1, tokens_2):\n",
        "  similarities = []\n",
        "  for token1 in tokens_1:\n",
        "    for token2 in tokens_2:\n",
        "      # Retrieve vectors for each pair of tokens\n",
        "      vector1 = get_vector(token1)\n",
        "      vector2 = get_vector(token2)\n",
        "\n",
        "      # Handling OOV words by assigning a similarity of 0.5\n",
        "      if vector1 is None or vector2 is None:\n",
        "        similarities.append(0.5)\n",
        "      else:\n",
        "        # Reshape the vectors for cosine similarity calculation\n",
        "        vector1 = vector1.reshape(1, -1)\n",
        "        vector2 = vector2.reshape(1, -1)\n",
        "\n",
        "        # Calculate cosine similarity and extract the scalar value from the matrix\n",
        "        cosine_similarity_matrix_value = cosine_similarity(vector1, vector2)[0, 0]\n",
        "\n",
        "        similarities.append(cosine_similarity_matrix_value)\n",
        "\n",
        "  return similarities"
      ],
      "metadata": {
        "id": "K20ShSyrQ1zo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iyJNxVHZ1sJC"
      },
      "outputs": [],
      "source": [
        "# Function to get the consine similarity matrix between two terms\n",
        "def get_cosine_similarity(term_1, term_2):\n",
        "  # Check if either of the term contains spaces, indicating multi-word phrases\n",
        "  if (' ' in term_1 or ' ' in term_2):\n",
        "      # Calculate the similarity for every pair of words in term_1 and term_2\n",
        "      term_1_tokens = term_1.split()\n",
        "      term_2_tokens = term_2.split()\n",
        "\n",
        "      similarities = calculate_similarity_matrix(term_1_tokens, term_2_tokens)\n",
        "\n",
        "      # All tokens are OOV, handling by returning a default similarity of 0.5\n",
        "      if not similarities:\n",
        "        return 0.5\n",
        "\n",
        "      # Return the average similarity for multi-word phrases\n",
        "      return np.mean(similarities)\n",
        "\n",
        "  else:\n",
        "    # Calculate similarity between single words\n",
        "    vector1 = get_vector(term_1)\n",
        "    vector2 = get_vector(term_2)\n",
        "\n",
        "    # Handling OOV words by returning a default similarity of 0.5\n",
        "    if vector1 is None or vector2 is None:\n",
        "      return 0.5\n",
        "\n",
        "    # Reshape the vectors for cosine similarity calculation\n",
        "    vector1 = vector1.reshape(1, -1)\n",
        "    vector2 = vector2.reshape(1, -1)\n",
        "\n",
        "    # Calculate cosine similarity and extract the scalar value from the matrix\n",
        "    cosine_similarity_matrix_value = cosine_similarity(vector1, vector2)[0, 0]\n",
        "\n",
        "    # Return the similarity between single words\n",
        "    return cosine_similarity_matrix_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uK23vZ3FFyJI"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "validation_df = pd.read_csv(\"./data/CW-1-testdata.csv\", header=None)\n",
        "\n",
        "validation_df.columns = [\"id\",\"term1\", \"term2\"]\n",
        "\n",
        "# Clean the input values\n",
        "validation_df[[\"term1\", \"term2\"]] = validation_df[[\"term1\", \"term2\"]].map(clean_text)\n",
        "\n",
        "# Produce the output\n",
        "validation_df[\"cosine_similarity\"] = validation_df.apply(lambda x: get_cosine_similarity(x[\"term1\"], x[\"term2\"]), axis=1)\n",
        "\n",
        "# Create a .csv file\n",
        "validation_df.to_csv(\"./data/10927437_task1_results.csv\", header=False, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}